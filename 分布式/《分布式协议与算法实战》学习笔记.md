# 学习分布式的简要记录

> 《分布式协议与算法实战》
>
> https://time.geekbang.org/column/intro/100046101

# 学习路径

## 分布式算法比对

![img](https://upyun.xuqiming.tech/knhub/1cc7514e341fab7bd7044b37285f4433.jpg)

> 一般而言，在可信环境（比如企业内网）中，系统具有故障容错能力就可以了，常见的算法有二阶段提交协议（2PC）、TCC（Try-Confirm-Cancel）、Paxos 算法、ZAB 协议、Raft 算法、Gossip 协议、Quorum NWR 算法。
>
> 而在不可信的环境（比如有人做恶）中，这时系统需要具备拜占庭容错能力，常见的拜占庭容错算法有 POW 算法、PBFT 算法。

- 强一致性：保证写操作完成后，任何后续访问都能读到更新后的值。
- 弱一致性：写操作完成后，系统不能保证后续的访问都能读到更新后的值。
- 最终一致性：保证如果对某个对象没有新的写操作了，最终所有后续访问都能读到相同的最近更新的值。

## 知识体系

![img](https://upyun.xuqiming.tech/knhub/122bdf34957c6277352ea51c43552213.png)

# 拜占庭将军问题

## 二忠一叛的难题

3个将军中有1个是叛徒，则无法保持一致性。

## 口信消息型拜占庭问题之解

如果叛将人数为 m，将军人数不能少于 3m + 1 ，那么拜占庭将军问题就能解决了。

这个算法有个前提，也就是叛将人数 m，或者说能容忍的叛将数 m，是已知的。在这个算法中，叛将数 m 决定递归循环的次数（也就是说，叛将数 m 决定将军们要进行多少轮作战信息协商），即 **m+1 轮**（所以，你看，只有楚是叛变的，那么就进行了两轮）。

你也可以从另外一个角度理解：**n 位将军，最多能容忍 (n - 1) / 3 位叛将**。

> 解决二忠一叛必须增加人数。

## 签名消息型拜占庭问题之解

通过签名来防止消息被篡改

> PBFT算法的实现基础
>
> 注：也需要进行m+1 轮

## 选择

在存在恶意节点行为的场景中（比如在数字货币的区块链技术中），必须使用拜占庭容错算法（Byzantine Fault Tolerance，BFT）。除了上述两种算法，常用的拜占庭容错算法还有：PBFT 算法，PoW 算法。

计算机分布式系统中，最常用的是非拜占庭容错算法，即故障容错算法（Crash Fault Tolerance，CFT）。CFT 解决的是分布式的系统中存在故障，但不存在恶意节点的场景下的共识问题。 也就是说，这个场景可能会丢失消息，或者有消息重复，但不存在错误消息，或者伪造消息的情况。常见的算法有 Paxos 算法、Raft 算法、ZAB 协议。

# CAP

## 概念

- 一致性（`Consistency`）

  强调的是数据正确，也就是说，对客户端而言，每次读都能读取到最新写入的数据。

- 可用性（`Availability`）

  强调的是服务可用，但不保证数据正确。

- 分区容错性（`Partition Tolerance`）

  强调的是集群对分区故障的容错能力

## 选择

3个指标不可能同时满足，只能同时满足其中两个。

**P必须满足：**只要有网络交互就一定会有延迟和数据丢失，而这种状况我们必须接受，还必须保证系统不能挂掉。所以，节点间的分区故障是必然发生的。也就是说，分区容错性（P）是前提，是必须要保证的。

> 注：其实，在不存在网络分区的情况下，也就是分布式系统正常运行时（这也是系统在绝大部分时候所处的状态），就是说在不需要 P 时，C 和 A 能够同时保证。

- CP 模型，采用 CP 模型的分布式系统，舍弃了可用性，一定会读到最新数据，不会读到旧数据。一旦因为消息丢失、延迟过高发生了网络分区，就影响用户的体验和业务的可用性（比如基于 Raft 的强一致性系统，此时可能无法执行读操作和写操作）。典型的应用是 Etcd，Consul 和 Hbase。

- AP 模型，采用 AP 模型的分布式系统，舍弃了一致性，实现了服务的高可用。用户访问系统的时候，都能得到响应数据，不会出现响应错误，但会读到旧数据。典型应用就比如 Cassandra 和 DynamoDB。

# 二阶段提交协议

> XA协议基于二阶段提交协议提出

## 过程

1. 提交请求阶段（又称投票阶段）
2. 提交执行阶段（又称完成阶段）

## 缺点

- 在提交请求阶段，需要预留资源，在资源预留期间，其他人不能操作（比如，XA 在第一阶段会将相关资源锁定）；
- 相比业务，数据库是独立的系统，很难通过编程修改核心代码。
- 协调者故障，参与者长期锁定资源

因为前两点，我们无法根据业务特点弹性地调整锁的粒度，而这些都会影响数据库的并发性能。

那用什么办法可以解决这些问题呢？答案就是 TCC。

# TCC

TCC 是 Try（预留）、Confirm（确认）、Cancel（撤销） 3 个操作的简称，它包含了预留、确认或撤销这 2 个阶段。

## 过程

1. 通知各节点预留资源
2. 如果全都预留成功，执行确认，否则执行撤销。

## 说明

TCC 本质上是补偿事务，它的核心思想是针对每个操作都要注册一个与其对应的确认操作和补偿操作（也就是撤销操作）。

 它是一个业务层面的协议，你也可以将 TCC 理解为编程模型，TCC 的 3 个操作是需要在业务代码中编码实现的，为了实现一致性，确认操作和补偿操作必须是等幂的，因为这 2 个操作可能会失败重试。

# 三阶段提交协议

三阶段提交协议，虽然针对二阶段提交协议的“协调者故障，参与者长期锁定资源”的痛点，通过引入了**询问阶段**和**超时机制**，来减少资源被长时间锁定的情况，不过这会导致集群各节点在正常运行的情况下，使用更多的消息进行协商，增加系统负载和响应延迟。

也正是因为这些问题，三阶段提交协议很少被使用。

# BASE

BASE 理论是 CAP 理论中的 AP 的延伸，是对互联网大规模分布式系统的实践总结，强调可用性。

核心就是**基本可用（Basically Available）**和**最终一致性（Eventually consistent）**。

**软状态（Soft state）**，描述的是实现服务可用性的时候系统数据的一种**过渡状态**，也就是说不同节点间，数据副本存在短暂的不一致。（个人理解：对最终一致性的另一种描述。）

## 基本可用

实现基本可用的 4 板斧：

- **流量削峰**

将访问请求错开，削弱请求峰值。

- **延迟响应**

在队列中排队等待处理，可能几分钟或十几分钟后，系统才开始处理，然后响应处理结果

- **体验降级**

比如用小图片来替代原始图片，通过降低图片的清晰度和大小，提升系统的处理能力。

- **过载保护**

比如把接收到的请求放在指定的队列中排队处理，如果请求等待时间超时了（假设是 100ms），这个时候直接拒绝超时请求；再比如队列满了之后，就清除队列中一定数量的排队请求，保护系统不过载，实现系统的基本可用。

此外：

- 故障隔离（出现故障，做到故障隔离，避免影响其他服务） 
- 弹性扩容（基于Metric和Monitor实现系统态势感知，做到弹性伸缩）

## 最终一致性

常用的有这样几种实现最终一致性的具体方式：

- **读时修复**：在读取数据时，检测数据的不一致，进行修复。

  比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据。

- **写时修复**：在写入数据，检测数据的不一致时，进行修复。

  比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败就将数据缓存下来，然后定时重传，修复数据的不一致性。

- **异步修复**：这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。

> **写时修复**不需要做数据一致性对比，性能消耗比较低，对系统运行影响也不大，所以推荐在实现最终一致性时优先实现这种方式。作者推荐同时实现自定义写一致性级别（比如 All、Quorum、One、Any）
>
> 而**读时修复**和**异步修复**因为需要做数据的一致性对比，性能消耗比较多，在开发实际系统时，需要尽量优化一致性对比的算法，降低性能消耗，避免对系统运行造成影响。

# PAXOS算法





---

--- 学习中 ---